{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed21934",
   "metadata": {},
   "source": [
    "# 1. 텍스트 데이터 다루기\n",
    "\n",
    "1-1. 들어가며<br>\n",
    "1-2. 전처리 : 자연어의 노이즈 제거 | 자연어에 포함된 대표적인 세 가지 노이즈 유형을 확인하고 해결 방법을 학습합니다.<br>\n",
    "1-3. 분산표현 : 바나나와 사과의 관계를 어떻게 표현할까? | 분산표현이 무엇인지 희소표현과 같이 학습합니다.<br>\n",
    "1-4. 토큰화 : 그녀는? 그녀+는? | 대표적인 토큰화 기법인 공백, 형태소 기반 토큰화에 대하여 학습합니다.<br>\n",
    "1-5. 토큰화 : 다른 방법들 | OOV 문제를 해결한 BPE 그리고 BPE를 변형한 WPM에 대하여 학습합니다.<br>\n",
    "1-6. 토큰에게 의미를 부여하기 | 토큰화 기법이 아닌 단어 간 의미를 찾는 대표적인 세 가지 유형을 학습합니다.<br>\n",
    "1-7. 마무리하며\n",
    "---\n",
    "### keyword\n",
    "1. 노이즈 제거\n",
    "    1. 문장부호\n",
    "    2. 대소문자\n",
    "    3. 특수문자\n",
    "2. 희소표현 vs 분산표현\n",
    "3. Tokenizing\n",
    "    1. 공백 기반: corpus.split()\n",
    "    2. 형태소 기반(Korean): **[KoNLPs](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)** ~ 5 classes\n",
    "    3. **OOV** problem\n",
    "        1. **[BPE](https://arxiv.org/pdf/1508.07909.pdf)**\n",
    "        2. **[WPM](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf)** ~ **[SentencePiece](https://github.com/google/sentencepiece)**\n",
    "        3. **[soynlps](https://github.com/lovit/soynlp)**\n",
    "4. Embedding\n",
    "    1. token간의 관계성 고려\n",
    "        1. **[Word2Vec](https://wikidocs.net/22660)**\n",
    "        2. **[FastText](https://brunch.co.kr/@learning/7)**\n",
    "    2. context 고려: Contextualized Word Embedding\n",
    "        1. **[ELMo](https://brunch.co.kr/@learning/12)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558e592",
   "metadata": {},
   "source": [
    "## 1-1. 들어가며\n",
    "---\n",
    "### 학습 목표\n",
    "- **분산 표현**을 직관적으로 이해하고 이를 설명할 수 있습니다.\n",
    "- 문장 데이터를 정제하는 방법을 사용할 수 있습니다.\n",
    "- **토큰화**의 여러 가지 기법들을 사용할 수 있습니다.\n",
    "- **단어 Embedding을 구축**하는 방법을 사용할 수 있습니다.\n",
    "---\n",
    "[ 알면 좋은... ]\n",
    "- 컴파일러\n",
    "- 형식언어와 오토마타 이론\n",
    "- 자연언어처리\n",
    "\n",
    "참고 : https://dukeyang.tistory.com/2\n",
    "\n",
    "\n",
    "`파싱(parsing)` : 파싱(parsing)은 **구문 분석**이라고 한다. <u>1)문장이 이루고 있는 구성 성분을 분해</u>하고 <u>2)분해된 성분의 위계 관계를 분석하여 구조를 결정</u>하는 것이다. 즉 데이터를 분해 분석하여 원하는 형태로 조립하고 다시 빼내는 프로그램을 말한다.\n",
    "\n",
    "`파서(parser)` : 파서 컴퓨팅에서 파서(parser)는 **인터프리터나 컴파일러의 구성 요소 가운데 하나**로, <u>입력 토큰에 내재된 자료 구조를 빌드하고 문법을 검사</u>한다. 파서는 일련의 입력 문자로부터 토큰을 만들기 위해 별도의 낱말 분석기를 이용하기도 한다.\n",
    "\n",
    "자연어는 문맥의존 문법(Context-sensitive Grammar), 프로그래밍 언어는 문맥자유 문법(Context-free Grammar)\n",
    "\n",
    "- 문맥의존 문법(Context-sensitive Grammar) : 해석에 real world knowledge 필요\n",
    "- 문맥자유 문법(Context-free Grammar) : 문맥과 상황에 따라 뜻이 달라지지 않음. 문맥을 완전히 배제하고 해석 가능\n",
    "\n",
    "\n",
    "### 언어 처리 기술,, 의미의 모호성\n",
    "\n",
    "- 자연언어와 프로그래밍 언어의 처리과정은 유사한 성격이 많음. \n",
    "- 컴파일러 번역과정은 매우 흡사함. \n",
    "- 렉시칼 분석 단계에서 변수나 연산자를 식별하는 과정 = 어휘 분석 단계에서 형태소들을 분리하고, 품사를 결정하는 과정\n",
    "- 컴파일러 구문 (syntax) 이용한 파싱 = 자연언어의 문법 규칙 (grammar rule) 을 이용한 파싱 \n",
    "- 자연언어는 문맥 의존 언어로 간주되나, 처리의 어려움을 고려하여 문맥 자유 문법을 기본틀로 삼아 처리. + 부가적으로 문맥 의존적 요소\n",
    "- 컴파일러의 인덱스 테이블 역활은 , 자연어 처리에서는 사전의 역활이 큼. \n",
    "- 크게 다른 것은 인위적인 설계가 아니기 때문에 두 가지 이상의 유효한 분석 결과를 내게 되는 모호성 현상이 있음. \n",
    "- '착한 영희의 친구' : 영희가 착한 건지 , 영희의 친구가 착한 건지 모호함.\n",
    "\n",
    "\n",
    "언어처리 기술 -> 이런 모호성을 처리하는데 역점을 둠,\n",
    "\n",
    "`파싱 트리` : https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%8A%A4_%ED%8A%B8%EB%A6%AC\n",
    "\n",
    "`구문 분석 파싱 트리` :\n",
    "\n",
    "........\n",
    "\n",
    "오토마타 이론, 컴파일러... 공부해야겠다\n",
    "\n",
    "---\n",
    "\n",
    "프로그래밍으로 파싱을 만드는 데에 도저히 해낼 수 없던 자연어 처리 테스크가 **머신러닝 기법(워드 벡터 기법 등)으로 가능**해짐 \n",
    "\n",
    "**토큰화(Tokenization)** 기법은 자연어처리 모델의 성능에 결정적 영향을 미칩니다. \n",
    "\n",
    "토크나이저가 문장을 단어로 **쪼개는 방식에 따라** <u>같은 문장도 완전히 다른 워드 벡터 리스트</u>가 되기 때문\n",
    "\n",
    "특히 `Wordpiece Model`같은 `Subword level`의 처리 기법은 최신 자연어처리 모델에 필수적으로 자리 잡고 있으므로, \n",
    "\n",
    "이러한 아이디어가 나오기까지의 자연어 전처리, 토큰화 과정을 이해하는 것은 매우 중요합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61369d77",
   "metadata": {},
   "source": [
    "## 1-2. 전처리 : 자연어의 노이즈 제거\n",
    "---\n",
    "- 자연어에 포함된 대표적인 **세 가지 노이즈 유형**을 확인하고 해결 방법을 학습합니다.\n",
    "\n",
    "자연어에서 노이즈? 자연어에서 노이즈란 \n",
    "\n",
    "- 원하는 정보와 관련이 없거나, \n",
    "- 잡음으로 인해 정보를 제대로 파악하기 어려운 데이터를 의미합니다. \n",
    "\n",
    "자연어에 포함된 노이즈는 매우 다양합니다. \n",
    "\n",
    "예를 들어, 문장 내에서 \n",
    "- 오타, \n",
    "- 철자 오류, \n",
    "- 문법 오류, \n",
    "- 다의어 등\n",
    "\n",
    "의 혼란 요소가 있을 수 있습니다. \n",
    "\n",
    "- 문맥이나 \n",
    "- 대화 상황에 따라 의미가 바뀌는 언어의 특성상 모호한 부분\n",
    "\n",
    "도 많이 존재합니다. \n",
    "\n",
    "- 비속어나 \n",
    "- 사투리, \n",
    "- 외래어 등\n",
    "\n",
    "도 자연어에 포함되는 노이즈 중 하나입니다.\n",
    "\n",
    "---\n",
    "이런 노이즈 문제 때문에, 소설, 신문 기사 등이 일반적인 데이터, 채팅 데이터등이 일반적이지 않은 데이터.<br>\n",
    "아직은 \"서비스 목적\"에 맞게 데이터 골라 사용. <br>\n",
    "위의 채팅 데이터들도 데이터가 아주 많이 쌓이면 해결 가능\n",
    "\n",
    "#### 채팅 데이터 노이즈 유형\n",
    "1. 불완전한 문장으로 구성된 대화의 경우\n",
    "2. 문장의 길이가 너무 길거나 짧은 경우\n",
    "3. 채팅 데이터에서 문장 시간 간격이 너무 긴 경우\n",
    "4. 바람직하지 않은 문장의 사용\n",
    "\n",
    "### 대표적인 노이즈 유형\n",
    "1. 문장부호 : Hi, my name is John. (\"Hi,\" \"my\", ..., \"John.\" 으로 분리됨)\n",
    "2. 대소문자 : First, open the first chapter. (First와 first를 다른 단어로 인식)\n",
    "3. 특수문자 : He is a ten-year-old boy. (ten-year-old를 한 단어로 인식)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5398400",
   "metadata": {},
   "source": [
    "### 노이즈 유형 (1) : 문장부호 : Hi, my name is john.\n",
    "\n",
    ": 문장부호 양쪽에 공백을 취함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec34d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ,  my name is john . \n"
     ]
    }
   ],
   "source": [
    "def pad_punctuation(sentence, punc):\n",
    "    for p in punc:\n",
    "        sentence = sentence.replace(p, \" \" + p + \" \")\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence = \"Hi, my name is john.\"\n",
    "\n",
    "print(pad_punctuation(sentence, [\".\", \"?\", \"!\", \",\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a7e0f",
   "metadata": {},
   "source": [
    "### 노이즈 유형 (2) : 대소문자 : First, open the first chapter.\n",
    "\n",
    ": 모든 단어를 소문자로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989c2dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first, open the first chapter.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"First, open the first chapter.\"\n",
    "\n",
    "print(sentence.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf451c6a",
   "metadata": {},
   "source": [
    "### 노이즈 유형 (3) 특수문자 : He is a ten-year-old boy.\n",
    "\n",
    ": 사용할 알파벳과 기호들을 정의해 이를 제외하곤 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf04c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a ten year old boy.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"He is a ten-year-old boy.\"\n",
    "sentence = re.sub(\"([^a-zA-Z.,?!])\", \" \", sentence)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262018ab",
   "metadata": {},
   "source": [
    "### 종합 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ed9ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
      "but my teacher had been with me several weeks before i understood that everything has a name . \n",
      "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
      "some one was drawing water and my teacher placed my hand under the spout .  \n",
      "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
      "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
      "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
      "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
      "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
      "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From The Project Gutenberg\n",
    "# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)\n",
    "\n",
    "corpus = \\\n",
    "\"\"\"\n",
    "In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk. \n",
    "But my teacher had been with me several weeks before I understood that everything has a name.\n",
    "One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered. \n",
    "Some one was drawing water and my teacher placed my hand under the spout. \n",
    "As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly. \n",
    "I stood still, my whole attention fixed upon the motions of her fingers. \n",
    "Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me. \n",
    "I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand. \n",
    "That living word awakened my soul, gave it light, hope, joy, set it free! \n",
    "There were barriers still, it is true, but barriers that could in time be swept away.\n",
    "\"\"\" \n",
    "\n",
    "def cleaning_text(text, punc, regex):\n",
    "    # 노이즈 유형 (1) 문장부호 공백추가\n",
    "    for p in punc:\n",
    "        text = text.replace(p, \" \" + p + \" \")\n",
    "\n",
    "    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거\n",
    "    text = re.sub(regex, \" \", text).lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "print(cleaning_text(corpus, [\".\", \",\", \"!\", \"?\"], \"([^a-zA-Z0-9.,?!\\n])\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44da79",
   "metadata": {},
   "source": [
    "## 1-3. 분산표현\n",
    "---\n",
    "\n",
    "**`희소 표현(Sparse representation)`** : 단어를 고정된 크기의 벡터로 표현하지 않고, 이진화(binary) 또는 빈도수(frequency) 등의 방식으로 표현하는 방식 <br>/ -> 워드 벡터의 대부분의 차원은 0.0으로 채워진 희소 표현이 만들어질 것\n",
    "- 문제점 : 너무 고차원이 필요하고 불필요한 메모리와 연산량이 낭비된다는 점 말고도 중요한 문제점 하나는, 희소 표현의 워드 벡터끼리는 단어들 간의 의미적 유사도를 계산할 수 없다는 점\n",
    "\n",
    "**`분산 표현(distributed representation)`** : 단어를 고정된 크기의 벡터로 표현하는 방식 <br>/ Embedding 레이어를 사용해 \"각 단어가 몇 차원의 속성을 가질지\" 정의하는 방식으로 단어의 분산 표현(distributed representation) 를 구현하는 방식을 주로 사용\n",
    "\n",
    "코사인 유사도(Cosine Similarity) : 두 고차원 벡터의 유사도 구하기\n",
    "\n",
    "- 희소표현 : 속성 값을 일일히 정해줌\n",
    "- 분산표현 : 수많은 텍스트 데이터를 읽어가며 적합한 값을 찾아갈 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378b17e",
   "metadata": {},
   "source": [
    "### 1) 희소 표현(Sparse representation)\n",
    "\n",
    "- 희소표현 방식의 단어 사전:\n",
    "```bash\n",
    "{\n",
    "    //     [성별, 연령]\n",
    "    남자:   [-1.0, 0.0], // 이를테면 0.0 이 \"관계없음 또는 중립적\" 을 의미할 수 있겠죠!\n",
    "    여자:   [ 1.0, 0.0],\n",
    "    소년:   [-1.0, -0.7],\n",
    "    소녀:   [ 1.0, -0.7],\n",
    "    할머니:  [ 1.0, 0.7],\n",
    "    할아버지: [-1.0, 0.7],\n",
    "    아저씨:  [-1.0, 0.2],\n",
    "    아줌마:  [ 1.0, 0.2]\n",
    "}\n",
    "```\n",
    "```bash\n",
    "{\n",
    "    //      [성별, 연령, 과일, 색깔]\n",
    "    남자:  [-1.0, 0.0, 0.0,  0.0],\n",
    "    여자:  [ 1.0, 0.0, 0.0,  0.0],\n",
    "    사과:  [ 0.0, 0.0, 1.0,  0.5],   // 빨갛게 잘 익은 사과\n",
    "    바나나: [ 0.0, 0.0, 1.0, -0.5]   // 노랗게 잘 익은 바나나\n",
    "}\n",
    "```\n",
    "\n",
    "공유하는 의미속성이 없는 위 두 벡터의 내적은 0이 되므로, 코사인 유사도도 역시 0이 됩니다. 이렇게 되면 희소표현으로 본 두 단어 사이에는 아무런 의미적 유사도가 없게 됩니다.\n",
    "\n",
    "-> Embedding 레이어를 사용해 각 단어가 몇 차원의 속성을 가질지 정의하는 방식으로 단어의 분산 표현(distributed representation) 를 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a79ad5",
   "metadata": {},
   "source": [
    "### 2) 분산 표현(distributed representation)\n",
    "\n",
    "희소 표현의 코사인유사도는 0이라 각 단어의 의미를 파악할 수 없음., 그래서:\n",
    "\n",
    "- **Embedding 레이어**를 사용해 각 단어가 **몇 차원의 속성을 가질지** 정의하는 방식으로 단어의 **분산 표현(distributed representation)** 을 구현하는 방식을 주로 사용하게 됩니다.\n",
    "- 만약 `100개의 단어`를 `256차원의 속성`으로 표현하고 싶다면 Embedding 레이어는 아래와 같이 정의되겠죠.\n",
    "\n",
    "```python\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=256)\n",
    "```\n",
    "\n",
    "위 단어의 분산 표현에는 우리가 일일이 정의할 수 없는 어떤 **추상적인 속성들이 \"256차원\" 안에 골고루 분산**되어 표현\n",
    "\n",
    "희소 표현처럼 속성값을 임의로 지정해 주는 것이 아니라, **수많은 텍스트 데이터를 읽어가며 적합한 값을 찾아갈 것**입니다. <br>\n",
    "적절히 훈련된 분산 표현 모델을 통해 우리는 단어 간의 의미 유사도를 계산하거나, <br>\n",
    "이를 feature로 삼아 복잡한 자연어처리 모델을 훈련시킬 수 있게 됩니다.\n",
    "\n",
    "\n",
    "- 단어 사전 구성과 활용의 문제\n",
    "\n",
    "`문장` --tokenizing--> `토큰` --분산표현으로embedding--> `단어사전 구성`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e7ded",
   "metadata": {},
   "source": [
    "## 1-4. 토큰화\n",
    "---\n",
    "\n",
    "1. 공백 기반 토큰화\n",
    "\n",
    "```\n",
    "tokens = corpus.split()\n",
    "```\n",
    "\n",
    "2. 형태소 기반 토큰화\n",
    "\n",
    "```python\n",
    "tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt()]\n",
    "\n",
    "kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n",
    "\n",
    "for tokenizer in tokenizer_list:\n",
    "    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe672a9",
   "metadata": {},
   "source": [
    "### 1) 공백 기반 토큰화\n",
    "\n",
    "`Hi`, 를 `Hi`와 `,`로 나누기 위해 문장부호 양옆에 공백을 추가해 주었습니다. <br>**공백 기반 토큰화**를 사용하기 위해서였죠! \n",
    "<br>당시의 예제 코드를 다시 가져와 공백을 기반으로 토큰화를 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c27c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = \\\n",
    "\"\"\"\n",
    "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
    "but my teacher had been with me several weeks before i understood that everything has a name . \n",
    "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
    "some one was drawing water and my teacher placed my hand under the spout .  \n",
    "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
    "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
    "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
    "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
    "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
    "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
    "\"\"\"\n",
    "\n",
    "tokens = corpus.split() # default인 space를 기준으로 나누어서 리스트에 담아줌\n",
    "\n",
    "print(\"문장이 포함하는 Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc8da0",
   "metadata": {},
   "source": [
    "문제점 : `day`, `days` 를 구분 못함 -> 불가피한 손실로 취급"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e0998",
   "metadata": {},
   "source": [
    "### 2) 형태소 기반 토큰화\n",
    "\n",
    "한국어의 경우 공백기반 토큰화는 문제가 큼 -> 형태소 기반 토큰화로 해결\n",
    "\n",
    "- 대표적인 한국어 형태소 분석기 : [KoNLPy](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)\n",
    "    - `KoNLPy`는 내부적으로 **5가지의 형태소 분석 Class**를 포함\n",
    "    - 5가지 : `Hannanum`, `Kkma`, `Komoran`, `Mecab` ,`Okt`\n",
    "    - [한국어 형태소 분석기 성능 비교](https://iostream.tistory.com/144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c8dcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hannanum] \n",
      "[('코로나바이러스', 'N'), ('는', 'J'), ('2019년', 'N'), ('12월', 'N'), ('중국', 'N'), ('우한', 'N'), ('에서', 'J'), ('처음', 'M'), ('발생', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('뒤', 'N'), ('전', 'N'), ('세계', 'N'), ('로', 'J'), ('확산', 'N'), ('되', 'X'), ('ㄴ', 'E'), (',', 'S'), ('새롭', 'P'), ('은', 'E'), ('유형', 'N'), ('의', 'J'), ('호흡기', 'N'), ('감염', 'N'), ('질환', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n",
      "[Kkma] \n",
      "[('코로나', 'NNG'), ('바', 'NNG'), ('이러', 'MAG'), ('슬', 'VV'), ('는', 'ETD'), ('2019', 'NR'), ('년', 'NNM'), ('12', 'NR'), ('월', 'NNM'), ('중국', 'NNG'), ('우', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('에', 'VV'), ('서', 'ECD'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKM'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), (',', 'SP'), ('새', 'NNG'), ('롭', 'XSA'), ('ㄴ', 'ETD'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n",
      "[Komoran] \n",
      "[('코로나바이러스', 'NNP'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNB'), ('12월', 'NNP'), ('중국', 'NNP'), ('우', 'NNP'), ('한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('뒤', 'NNG'), ('전', 'MM'), ('세계로', 'NNP'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETM'), (',', 'SP'), ('새롭', 'VA'), ('ㄴ', 'ETM'), ('유형', 'NNP'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNP'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
      "[Mecab] \n",
      "[('코로나', 'NNP'), ('바이러스', 'NNG'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNBC'), ('12', 'SN'), ('월', 'NNBC'), ('중국', 'NNP'), ('우한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('한', 'XSV+ETM'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKB'), ('확산', 'NNG'), ('된', 'XSV+ETM'), (',', 'SC'), ('새로운', 'VA+ETM'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('입니다', 'VCP+EF'), ('.', 'SF')]\n",
      "[Okt] \n",
      "[('코로나바이러스', 'Noun'), ('는', 'Josa'), ('2019년', 'Number'), ('12월', 'Number'), ('중국', 'Noun'), ('우한', 'Noun'), ('에서', 'Josa'), ('처음', 'Noun'), ('발생', 'Noun'), ('한', 'Josa'), ('뒤', 'Noun'), ('전', 'Noun'), ('세계', 'Noun'), ('로', 'Josa'), ('확산', 'Noun'), ('된', 'Verb'), (',', 'Punctuation'), ('새로운', 'Adjective'), ('유형', 'Noun'), ('의', 'Josa'), ('호흡기', 'Noun'), ('감염', 'Noun'), ('질환', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 다섯가지 형태소 분석기 비교\n",
    "\n",
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt\n",
    "\n",
    "tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt()]\n",
    "\n",
    "kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n",
    "\n",
    "for tokenizer in tokenizer_list:\n",
    "    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f443f",
   "metadata": {},
   "source": [
    "### 사전에 없는 단어의 문제: OOV(Out-Of-Vocabulary)\n",
    "\n",
    "지금까지 배운 **공백 기반**이나, **형태소 기반**의 토큰화 기법들은 모두 <u>**의미를 가지는 단위로 토큰을 생성**</u>합니다.\n",
    "<br> 이 기법의 경우, 데이터에 포함되는 모든 단어를 처리할 수는 없기 때문에 \n",
    "<br> **자주 등장한 상위 N개의 단어만 사용**하고 나머지는 :\n",
    "- <unk>같은 특수한 토큰(Unknown Token)으로 치환 ~> 불가피한 손실..\n",
    "    \n",
    "새로 등장한(본 적 없는) 단어에 대해 약한 모습을 보일 수밖에 없는 기법들이기에, 이를 해결하고자 하는 시도들이 있었습니다. 그리고 그것이 우리가 다음 스텝에서 배울, Wordpiece Model이죠!    \n",
    "    \n",
    "`Wordpiece Model`: OOV 문제 해결하고자 하는 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69fae7",
   "metadata": {},
   "source": [
    "## 1-5. 토큰화 : 다른 방법들\n",
    "---\n",
    "\n",
    "1. `Byte Pair Encoding(BPE)` : 데이터에서 가장 많이 등장하는 `바이트 쌍(Byte Pair)` 을 **새로운 단어로 치환하여 압축**하는 작업을 반복하는 방식으로 동작\n",
    "\n",
    "- `Wordpiece Model(WPM)` : 한 단어를 여러 개의 Subword의 집합으로 보는 방법\n",
    "    - 두 단어 `preview`와 `predict`를 보면 접두어인 `pre`가 공통되고 있죠? <br>`pre`가 들어간 단어는 주로 `\"미리\"`, `\"이전의\"` 와 연계되는 의미를 가지고 있습니다. <br>컴퓨터도 두 단어를 따로 볼 게 아니라 `pre+view`와 `pre+dict`로 본다면 학습을 더 잘 할 수 있지 않을까요?\n",
    "\n",
    "\n",
    "- `soynlp` : 한국어 자연어 처리를 위한 라이브러리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7c388",
   "metadata": {},
   "source": [
    "### 1) Byte Pair Encoding(BPE)\n",
    "\n",
    "- `Byte Pair Encoding(BPE)` : 데이터에서 가장 많이 등장하는 `바이트 쌍(Byte Pair)` 을 **새로운 단어로 치환하여 압축**하는 작업을 반복하는 방식으로 동작\n",
    "    - 예시:\n",
    "    ```bash\n",
    "    aaabdaaabac # 가장 많이 등장한 바이트 쌍 \"aa\"를 \"Z\"로 치환합니다.\n",
    "    → \n",
    "    ZabdZabac   # \"aa\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.\n",
    "    Z=aa        # 그다음 많이 등장한 바이트 쌍 \"ab\"를 \"Y\"로 치환합니다.\n",
    "    → \n",
    "    ZYdZYac     # \"ab\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.\n",
    "    Z=aa        # 여기서 작업을 멈추어도 되지만, 치환된 바이트에 대해서도 진행한다면\n",
    "    Y=ab        # 가장 많이 등장한 바이트 쌍 \"ZY\"를 \"X\"로 치환합니다.\n",
    "    → \n",
    "    XdXac\n",
    "    Z=aa\n",
    "    Y=ab\n",
    "    X=ZY       # 압축이 완료되었습니다!\n",
    "    ```\n",
    "    - 모든 단어를 **문자(바이트)들의 집합으로 취급**하여 자주 등장하는 문자 쌍을 합치면, 접두어나 접미어의 의미를 캐치할 수 있고, **처음 등장하는 단어는 문자(알파벳)들의 조합**으로 나타내어 OOV 문제를 완전히 해결할 수 있다는 것이죠!<br><br>\n",
    "\n",
    "    - BPE는 단어를 기본 단위인 **문자(character) 단위로 분해**한 후, 가장 자주 등장하는 문자의 쌍(pair)을 하나의 문자로 대체하는 방식으로 동작\n",
    "    - 이를 통해 기존에 없던 단어가 등장하더라도, <u>해당 단어를 구성하는 문자들의 조합이 기존에 등장했던 문자 쌍과 일치하는 경우</u>,\n",
    "    - 기존에 등장했던 토큰들의 조합으로 변환하여 OOV 문제를 해결할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d992d6",
   "metadata": {},
   "source": [
    "[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf) : 기계번역에 BPE 사용 제안 논문\n",
    "\n",
    "위 논문에 있는 파이썬 소스코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed521647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "# 임의의 데이터에 포함된 단어들입니다.\n",
    "# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.\n",
    "vocab = {\n",
    "    'l o w '      : 5,\n",
    "    'l o w e r '  : 2,\n",
    "    'n e w e s t ': 6,\n",
    "    'w i d e s t ': 3\n",
    "}\n",
    "\n",
    "num_merges = 5\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    단어 사전을 불러와\n",
    "    단어는 공백 단위로 쪼개어 문자 list를 만들고\n",
    "    빈도수와 쌍을 이루게 합니다. (symbols)\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        for i in range(len(symbols) - 1):              # 모든 symbols를 확인하여 \n",
    "            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다. \n",
    "        \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    문자 쌍(pair)과 단어 리스트(v_in)를 입력받아\n",
    "    각각의 단어에서 등장하는 문자 쌍을 치환합니다.\n",
    "    (하나의 글자처럼 취급)\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "        \n",
    "    return v_out, pair[0] + pair[1]\n",
    "\n",
    "\n",
    "token_vocab = []\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(\">> Step {0}\".format(i + 1))\n",
    "    \n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.\n",
    "    vocab, merge_tok = merge_vocab(best, vocab)\n",
    "    print(\"다음 문자 쌍을 치환:\", merge_tok)\n",
    "    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n",
    "    \n",
    "    token_vocab.append(merge_tok)\n",
    "    \n",
    "print(\"Merged Vocab:\", token_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0130efd",
   "metadata": {},
   "source": [
    "만일 `lowest`라는 처음 보는 단어가 등장하더라도, 위 알고리즘을 따르면 **어느 정도 의미가 파악된** `low`와 `est`의 결합으로 표현할 수 있습니다. 또 BPE의 놀라운 점은 <u>아무리 큰 데이터도 원하는 크기로 OOV 문제없이 사전을 정의할 수 있다는 것</u>입니다. <br>극단적으로 생각했을 때 알파벳 26개와 특수문자, 문장부호를 아무리 추가해도 100개 이내로 사전을 정의할 수 있죠.(물론 그러면 안 됩니다!!)\n",
    "\n",
    "Embedding 레이어는 **단어의 개수 x Embedding 차원 수** 의 Weight를 생성하기 때문에 단어의 개수가 줄어드는 것은 곧 메모리의 절약으로 이어집니다. 많은 데이터가 곧 정확도로 이어지기 때문에 이런 기여는 굉장히 의미가 있습니다!\n",
    "\n",
    "문제점 : 만약 수많은 데이터를 사용해 만든 BPE 사전으로 모델을 학습시키고 문장을 생성하게 했다고 합시다. \n",
    "- 그게 [i, am, a, b, o, y, a, n, d, you, are, a, gir, l]이라면, \n",
    "- **어떤 기준으로** 이들을 결합해서 문장을 복원하죠? 몽땅 한꺼번에 합쳤다간 끔찍한 일이 벌어질 것만 같습니다...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab104a",
   "metadata": {},
   "source": [
    "### 2) Wordpiece Model(WPM)\n",
    "\n",
    "BPE와의 두가지 차별점 :\n",
    "1. **공백 복원**을 위해 단어의 <u>시작 부분</u>에 언더바 `_` 를 추가합니다.\n",
    "    1. 모든 토큰을 합친 후, \n",
    "    2. 언더바 _를 공백으로 치환\n",
    "2. 빈도수 기반이 아닌 **[가능도(Likelihood)](https://jjangjjong.tistory.com/41)를 증가시키는 방향**으로 문자 쌍을 합칩니다. (더 *'그럴듯한'* 토큰을 만들어냅니다.)\n",
    "\n",
    "\n",
    "WPM 관련 논문: [JAPANESE AND KOREAN VOICE SEARCH](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf)\n",
    "- 의의:\n",
    "    - 조사, 어미 등의 활용이 많고 복잡한 한국어 같은 모델의 토크나이저로 WPM이 좋은 대안이 될 수 있다.\n",
    "    - WPM은 어떤 언어든 무관하게 적용 가능한 language-neutral하고 general한 기법이다. <br>한국어 형태소 분석기처럼 한국어에만 적용 가능한 기법보다 훨씬 활용도가 크다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830f255",
   "metadata": {},
   "source": [
    "WPM은 현재 비공개,, 대신 구글의 **`SentencePiece` [라이브러리](https://github.com/google/sentencepiece)**를 통해 \n",
    "- 고성능의 BPE를 사용할 수 있습니다! SentencePiece에는 \n",
    "- **전처리 과정도 포함**되어 있어서, 데이터를 따로 정제할 필요가 없어 간편하기까지 합니다.\n",
    "\n",
    "이제 우리는 어떤 언어에도 OOV 발생 우려 없이 안정적으로 활용할 수 있는 멋진 <u>토크나이징 기술(SentencePiece..?)</u>을 확보했습니다. 이제는 컴퓨터가 단어사전을 안심하고 활용할 수 있겠군요!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8577c",
   "metadata": {},
   "source": [
    "### 3) soynlp: 한국어를 위한 tokenizer\n",
    "\n",
    "- `soynlp`: 한국어 자연어 처리를 위한 [라이브러리](https://github.com/lovit/soynlp)\n",
    "    - 토크나이저 외에도 \"단어 추출\", \"품사 판별\", \"전처리 기능\"도 제공\n",
    "    \n",
    "형태소 기반의 토크나이저가 미등록 단어에 취약하기 때문에 WordPiece Model을 사용하는 것처럼, <br>\n",
    "형태소 기반인 koNLPy의 단점을 해결하기 위해 soynlp를 사용할 수 있습니다.\n",
    "\n",
    "- **비지도학습**을 통해 training data에 없는 데이터의 **단어의 경계**(를 알아야 올바르게 토크나이징 가능)를 결정하여 토크나이징\n",
    "- 여기서 비지도 학습 방식이 \" 통계적인 방식 \" -> soynlp를 \"통계기반 토크나이저\"로 분류하기도 함\n",
    "\n",
    "예를들어 : `트와이스`가 한 단어임을 인지하기 위해서 `트`, `트와`, `트와이`, `트와이스` 각각 다음 글자의 확률을 계산해서 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcde74",
   "metadata": {},
   "source": [
    "그럼 토크나이징 배웠고,, \n",
    "\n",
    "단어의 분산 표현을 얻는 법?\n",
    "\n",
    "한국어라면 `자동차`를 `_자동 / 차` 로 분리되는데... <br>\n",
    "속성이 아무리 추상적이래도 보기에 차가 **마시는 차**인지, **달리는 차**인지 도통 알 수가 없죠? <br>\n",
    "게다가 설령 토큰화가 완벽하다고 해도, `남자`가 `[-1, 0]`인지 `[1, 0]`인지는 컴퓨터 입장에서는 알 도리가 없습니다.\n",
    "\n",
    "Embedding 레이어는 선언 즉시 랜덤한 실수로 Weight 값을 채우고, 학습을 진행하며 적당히 튜닝해가는 방식으로 속성을 맞춰가지만 이는 뭔가 찜찜합니다. \n",
    "\n",
    "토큰들이 멋지게 의미를 갖게 하는 방법은 없을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66b8ff",
   "metadata": {},
   "source": [
    "## 1-6. 토큰에게 의미를 부여하기 \n",
    "---\n",
    "\n",
    ": 토큰화 기법이 아닌 단어 간 의미를 찾는 대표적인 세 가지 유형을 학습합니다.\n",
    "\n",
    "각 토큰들이 랜덤하게 부여된 실수로 살아가지 않게, 그들끼리 **유사도 연산**을 할 수 있게 <u>의미를 부여하는 알고리즘</u>, 당연히 있습니다! 심지어 토큰화 기법보다 더 많이요! 대표적인 3가지만 알아보도록 하겠습니다. 이번 코스에선 간단하게 아이디어만 이해해도 충분합니다.\n",
    "\n",
    "1. Word2Vec\n",
    "2. FastText\n",
    "3. ELMo - the 1st Contextualized Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdf9e1",
   "metadata": {},
   "source": [
    "### 1) Word2Vec\n",
    "\n",
    "`난 오늘 술을 한 잔 마셨어` 라는 문장의 \"각 단어\" \n",
    "즉, 동시에 등장하는 단어끼리는 연관성이 있다는 아이디어로 시작된 알고리즘입니다. \n",
    "\n",
    "예문의 경우 다른 단어는 몰라도 `술`과 `마셨어`는 괜찮은 연관성을 캐치해낼 수 있겠네요.\n",
    "\n",
    "- [위키독스/Word2Vec](https://wikidocs.net/22660)\n",
    "\n",
    "Word2Vec의 두가지 방식:\n",
    "1. **CBOW(Continuous Bag of Words)** : 주변에 있는 단어들을 입력으로 \"중간에 있는 단어\"들을 예측하는 방법\n",
    "2. **Skip-gram** : 중간에 있는 단어들을 입력으로 \"주변 단어\"들을 예측하는 방법\n",
    "\n",
    "- (논리적으론 CBOW가 좋아 보이지만) Skip-gram이 실제 실험에선 다소 우세하다.\n",
    "\n",
    "Word2Vec이 딥러닝이 아닌 이유: 은닉층이 1개인 경우 Deep Neural Network가 아닌 Shallow Neural Network라 칭한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a38119",
   "metadata": {},
   "source": [
    "### 2) FastText\n",
    "\n",
    "Word2Vec은 정말 좋은 방법이지만, 연산의 빈부격차가 존재\n",
    "\n",
    "자주 등장하지 않는 단어는 최악의 경우 **단 한 번의 연산만을 거쳐** 랜덤하게 초기화된 값과 크게 다르지 않은 상태로 알고리즘이 종료될 수 있습니다. \n",
    "\n",
    "`FastText`는 이를 해결하기 위해 `BPE`와 비슷한 아이디어를 적용했습니다.\n",
    "\n",
    "[한국어를 위한 어휘 임베딩의 개발 -1-:글 말미의 Fasttext의 등장 (2016) 파트](https://brunch.co.kr/@learning/7)\n",
    "\n",
    "\n",
    "기존의 단어마다 Embedding을 할당하던 방식과 FastText의 큰 차이점<br>\n",
    " : FastText는 한 단어를 `n-gram의 집합`이라고 보고 단어를 쪼개어 **각 n-gram에 할당된 Embedding의 평균값**을 사용하였다.\n",
    "- 어휘를 구성하는 n-gram vector의 평균 vector를 **어휘 임베딩**으로 봄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce78709",
   "metadata": {},
   "source": [
    "### 3) ELMo - the 1st Contextualized Word Embedding\n",
    "\n",
    "위의 알고리즘(Word2Vec, FastText)의 문제점: **\"고정적\"** ~ 동음의의어를 처리할 수 없음(Context)\n",
    "\n",
    "예시:\n",
    "- 탐스럽고 먹음직스러웠던 **사과**가 이렇게 썩어버리다니 너무 아쉽네요.\n",
    "- 당신이 저지른 실수는 **사과**한다고 용서될 수 있는게 아닙니다.\n",
    "\n",
    " '사과'의 context가 되는 것은 무엇일까요? \n",
    "- 첫 문장이라면 `탐스럽고 먹음직스러운` 이 될 것이고 \n",
    "- 다음 문장이라면 `이렇게 썩어버리다니` 가 될 것입니다.\n",
    "\n",
    "즉, 단어의 의미 벡터를 구하기 위해서는 그 단어가 사용된 **주변 단어의 맥락**을 넘어서 **사용된 시퀀스 전체의 맥락**이 함께 고려되는 `Word Embedding`이 필요한 것입니다. \n",
    "\n",
    "이런 개념을 **`Contextualized Word Embedding`**이라고 합니다.\n",
    "\n",
    "\n",
    "ELMo라는 모델은 \n",
    "- \"데이터에 단어가 등장한 순간\", \n",
    "- 그 \"주변 단어 정보\"를 사용해 Embedding을 구축\n",
    "\n",
    "하는 개념을 처음 소개하면서 자연어처리의 획기적인 발전의 계기를 마련해 준 \n",
    "- \"첫 번째 Contextualized Word Embedding 모델\"입니다. \n",
    "\n",
    "[전이 학습 기반 NLP (1): ELMo](https://brunch.co.kr/@learning/12)\n",
    "\n",
    "`bi-directional LSTM`을 활용한 `ELMo 모델`에서 `Contextual Word Embedding`이 되는 벡터는 **어떤 벡터 3가지**를 합쳐서 얻어지나요?\n",
    "1. 기존 어휘 임베딩(입력 토큰의 word vector)\n",
    "2. 순방향 LSTM의 hidden state vector\n",
    "3. 역방향 LSTM의 hidden state vector\n",
    "\n",
    "를 `concatenate`한 벡터가 ELMo의 `Contextual Word Embedding`이 됩니다.\n",
    "\n",
    "그러니까, ELMo 모델에서 처음 소개된 `Contextualized Word Embedding`가 bi-directional LSTM의 1)순방향, 2)역방향 vector를 3)입력토큰 임베딩벡터에 적용, concat시켜서 앞뒤 문맥정보를 포함시키는 알고리즘이다?!<br> Contextualized Word Embedding : ELMo, BERT 등 많이 사용되는 중"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9932f8",
   "metadata": {},
   "source": [
    "## 1-7. 마무리하며\n",
    "---\n",
    "- 토큰화\n",
    "- 분산표현\n",
    "\n",
    "문장이 입력되면 적절히 `토큰화`를 하고 토큰을 `임베딩(Embedding)`을 통해 분산 표현으로 만드는 것이지요.<br>\n",
    "분산 표현은 \"벡터\"이므로 이제 인공지능에 활용할 수 있습니다.\n",
    "\n",
    "- 토큰화에 사용되는 방법은 언어마다 다름(문장 구성 성분이 다르기 때문)\n",
    "- 임베딩을 할 때는 토큰마다 독립적으로 만들지 않고 \"토큰 간의 관계성을 주입\"합니다. 그래야 문장을 구성할 때 적절히 사용될 수 있기 때문이에요.\n",
    "\n",
    "\n",
    "- 이렇게 토큰 간의 관계성을 고려해서 만든 것으로는 Word2Vec, FastText 등이 있어요. \n",
    "- 거기다가 문장의 문맥까지 고려하는 ELMo까지 등장했습니다.\n",
    "\n",
    "\n",
    "\n",
    "자연어 처리 분야는 인공지능 모델도 중요하지만 토큰화와 임베딩에 관련된 내용이 매우 중요합니다.\n",
    "\n",
    "---\n",
    "---\n",
    "### 내용정리\n",
    "\n",
    "1. 노이즈 제거\n",
    "    1. 문장부호\n",
    "    2. 대소문자\n",
    "    3. 특수문자\n",
    "2. 희소표현 vs 분산표현\n",
    "3. Tokenizing\n",
    "    1. 공백 기반: corpus.split()\n",
    "    2. 형태소 기반(Korean): **[KoNLPs](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)** ~ 5 classes\n",
    "    3. **OOV** problem\n",
    "        1. **[BPE](https://arxiv.org/pdf/1508.07909.pdf)**\n",
    "        2. **[WPM](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf)** ~ **[SentencePiece](https://github.com/google/sentencepiece)**\n",
    "        3. **[soynlps](https://github.com/lovit/soynlp)**\n",
    "4. Embedding\n",
    "    1. token간의 관계성 고려\n",
    "        1. **[Word2Vec](https://wikidocs.net/22660)**\n",
    "        2. **[FastText](https://brunch.co.kr/@learning/7)**\n",
    "    2. context 고려: Contextualized Word Embedding\n",
    "        1. **[ELMo](https://brunch.co.kr/@learning/12)**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
