{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df15b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99764325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path =\"~/aiffel/dktc/data/train.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b229d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASS_NAMES)\n",
    "\n",
    "train_data['class'] = encoder.transform(train_data['class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2af7214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>392</td>\n",
       "      <td>1</td>\n",
       "      <td>병신이 아이스크림 먹게 돼 있냐?\\n난 먹으면 안 돼? 그만 좀 해.\\n당연히 안 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>523</td>\n",
       "      <td>2</td>\n",
       "      <td>과장님. 저 이번에 휴가 좀 갔다와도 되겠습니까.?\\n휴가? 왜??\\n좀 쉬다가 오...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>789</td>\n",
       "      <td>1</td>\n",
       "      <td>야 얘 이 쪽 손가락 세 개밖에 없다\\n엥 손가락이 세개밖에 없을 수가 있어?\\n봐...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>824</td>\n",
       "      <td>1</td>\n",
       "      <td>이야 내동생 여자친구한테 편지쓰네?\\n 아 형 돌려줘.\\n 어디보자. 사랑하는 여친...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>869</td>\n",
       "      <td>2</td>\n",
       "      <td>지현씨 나 소개팅 좀 시켜줘봐\\n네? 저 주변에 아는 사람이 없어서요\\n아 상사라 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>3797</td>\n",
       "      <td>1</td>\n",
       "      <td>너 이번 방학 때 쌍꺼풀 수술 하고왔지?\\n아닌데?\\n아니긴 뭐가 아니야. 눈이 이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>3798</td>\n",
       "      <td>1</td>\n",
       "      <td>안녕하세요 지금 먹방 촬영중인데 촬영가능할까요?\\n안돼요\\n한번만 안될까요?\\n안돼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3855</th>\n",
       "      <td>3855</td>\n",
       "      <td>1</td>\n",
       "      <td>그 소문 진짜야? 너가 다른 애들 뒷담화하고 다녔다며?\\n응? 나 그런 적 없는데?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>3874</td>\n",
       "      <td>1</td>\n",
       "      <td>야 니 왤캐 못생겼냐?\\n뭐라그랬냐?\\n으 나 보고 말하지마 니 얼굴보면 토나올거 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>3928</td>\n",
       "      <td>1</td>\n",
       "      <td>새파랗게 젊은게 어디 여길 앉아있어\\n저 임산부에요\\n사지 멀쩡한게! 임신이 벼슬이...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  class                                       conversation\n",
       "392    392      1  병신이 아이스크림 먹게 돼 있냐?\\n난 먹으면 안 돼? 그만 좀 해.\\n당연히 안 ...\n",
       "523    523      2  과장님. 저 이번에 휴가 좀 갔다와도 되겠습니까.?\\n휴가? 왜??\\n좀 쉬다가 오...\n",
       "789    789      1  야 얘 이 쪽 손가락 세 개밖에 없다\\n엥 손가락이 세개밖에 없을 수가 있어?\\n봐...\n",
       "824    824      1  이야 내동생 여자친구한테 편지쓰네?\\n 아 형 돌려줘.\\n 어디보자. 사랑하는 여친...\n",
       "869    869      2  지현씨 나 소개팅 좀 시켜줘봐\\n네? 저 주변에 아는 사람이 없어서요\\n아 상사라 ...\n",
       "...    ...    ...                                                ...\n",
       "3797  3797      1  너 이번 방학 때 쌍꺼풀 수술 하고왔지?\\n아닌데?\\n아니긴 뭐가 아니야. 눈이 이...\n",
       "3798  3798      1  안녕하세요 지금 먹방 촬영중인데 촬영가능할까요?\\n안돼요\\n한번만 안될까요?\\n안돼...\n",
       "3855  3855      1  그 소문 진짜야? 너가 다른 애들 뒷담화하고 다녔다며?\\n응? 나 그런 적 없는데?...\n",
       "3874  3874      1  야 니 왤캐 못생겼냐?\\n뭐라그랬냐?\\n으 나 보고 말하지마 니 얼굴보면 토나올거 ...\n",
       "3928  3928      1  새파랗게 젊은게 어디 여길 앉아있어\\n저 임산부에요\\n사지 멀쩡한게! 임신이 벼슬이...\n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.duplicated([\"class\", \"conversation\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa6bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop_duplicates(subset=[\"class\", \"conversation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264d33ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [idx, class, conversation]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.duplicated([\"conversation\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a94417aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...\n",
       "Name: conversation, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = train_data[\"conversation\"]\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5d89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def generate_spm_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3):\n",
    "    # sentencepiece train을 위해 해당 corpus를 별도 파일로 만듬.\n",
    "    temp_file = f\"{lang}_corpus.txt\"\n",
    "    with open(temp_file, \"w\") as f:\n",
    "        for sen in corpus:\n",
    "            f.write(sen)\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    #  sentencepiece train\n",
    "    model_name = f\"{lang}_spm\"\n",
    "    spm.SentencePieceTrainer.train(input=temp_file, \n",
    "                                   model_prefix=model_name, \n",
    "                                   vocab_size=vocab_size, \n",
    "                                   pad_id=pad_id, \n",
    "                                   bos_id=bos_id, \n",
    "                                   eos_id=eos_id, \n",
    "                                   unk_id=unk_id)\n",
    "    \n",
    "    #  sentencepiece load\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(model_name + \".model\")\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6051ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎ가-힣ㅏ-ㅣ?.!,]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dede1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [preprocess_sentence(s) for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f464f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지금 너 스스로를 죽여달라고 애원하는 것인가 ? 아닙니다 . 죄송합니다 . 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 ? 진짜 죽여버리고 싶게 . 정말 잘못했습니다 . 너가 선택해 . 너가 죽을래 네 가족을 죽여줄까 . 죄송합니다 . 정말 잘못했습니다 . 너에게는 선택권이 없어 . 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야 . 선택 못하겠습니다 . 한번만 도와주세요 . 그냥 다 죽여버려야겠군 . 이의 없지 ? 제발 도와주세요 .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72020c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3225  6564   343 ...    29  1061     2]\n",
      " [    2 15495  2348 ... 15501   121     2]\n",
      " [15512  9163 15513 ...   140  9164     4]\n",
      " ...\n",
      " [ 3024 50955 50956 ...  2652   209     3]\n",
      " [    2    48  3469 ...     2  1115     2]\n",
      " [ 2702  8746   585 ... 50977     2     7]] <keras_preprocessing.text.Tokenizer object at 0x7f833ceb08e0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=20)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e01eedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50977"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "014af1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = list(map(lambda s: len(s), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "576f3546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "909"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2a93377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6a93872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ko_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ko_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ko_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 3846 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=915069\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1163\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 3846 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 41588 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 3846\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 50913\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 50913 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=23731 obj=10.5052 num_tokens=100532 num_tokens/piece=4.23632\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=20973 obj=9.74399 num_tokens=100863 num_tokens/piece=4.80918\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=15728 obj=9.83113 num_tokens=106071 num_tokens/piece=6.74409\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=15718 obj=9.79192 num_tokens=106118 num_tokens/piece=6.75137\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11788 obj=10.0092 num_tokens=113440 num_tokens/piece=9.62335\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11787 obj=9.96744 num_tokens=113472 num_tokens/piece=9.62688\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11000 obj=10.0286 num_tokens=115164 num_tokens/piece=10.4695\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11000 obj=10.0189 num_tokens=115184 num_tokens/piece=10.4713\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ko_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ko_spm.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "spm_tokenizer = generate_spm_tokenizer(corpus, vocab_size)\n",
    "tokenized_corpus = []\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in range(len(corpus)):\n",
    "    tokenized_corpus.append(spm_tokenizer.EncodeAsIds(corpus[idx]))\n",
    "    \n",
    "len_tokenized = [map(lambda s: len(s), tokenized_corpus)]\n",
    "import seaborn as sns\n",
    "sns.histplot(len_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9539c8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "193/193 [==============================] - 4s 10ms/step - loss: 1.2550 - accuracy: 0.3771 - val_loss: 1.0013 - val_accuracy: 0.5052\n",
      "Epoch 2/10\n",
      "193/193 [==============================] - 1s 7ms/step - loss: 0.9872 - accuracy: 0.5244 - val_loss: 0.9848 - val_accuracy: 0.5078\n",
      "Epoch 3/10\n",
      "184/193 [===========================>..] - ETA: 0s - loss: 0.8189 - accuracy: 0.6104"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_len = 100\n",
    "X_train, X_val, y_train, y_val = train_test_split(tokenized_corpus, train_data['class'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, padding='post', maxlen=max_len)\n",
    "\n",
    "inputs = keras.layers.Input(shape=(None,))\n",
    "x = keras.layers.Embedding(vocab_size, 128)(inputs)\n",
    "# x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "x = keras.layers.LSTM(128)(x)\n",
    "x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "x = keras.layers.Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, x)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=16, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c6bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25fa7c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_000</th>\n",
       "      <th>t_001</th>\n",
       "      <th>t_002</th>\n",
       "      <th>t_004</th>\n",
       "      <th>t_005</th>\n",
       "      <th>t_006</th>\n",
       "      <th>t_007</th>\n",
       "      <th>t_009</th>\n",
       "      <th>t_010</th>\n",
       "      <th>t_012</th>\n",
       "      <th>...</th>\n",
       "      <th>t_489</th>\n",
       "      <th>t_490</th>\n",
       "      <th>t_492</th>\n",
       "      <th>t_493</th>\n",
       "      <th>t_494</th>\n",
       "      <th>t_495</th>\n",
       "      <th>t_496</th>\n",
       "      <th>t_497</th>\n",
       "      <th>t_498</th>\n",
       "      <th>t_499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "      <td>그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...</td>\n",
       "      <td>그 사람 누구냐?  누구말하는거야?  다 알면서 모른척 하지마. 둘다 쏴버리기 전에...</td>\n",
       "      <td>야 저기야 닭꼬치 트럭왔다 응 그러네 그치? 너도 먹고 싶지? 어?나도? 그래 너 ...</td>\n",
       "      <td>야 너 2학년 김민석 맞지? 네 맞는데요. 혹시 누구신가요? 내가 누군지 궁금하면 ...</td>\n",
       "      <td>예전에 네가 나한테 했던 일 기억하지? 너도 당해봐 예전 일이라면 내가 사과할게. ...</td>\n",
       "      <td>김비서 지금 우리애 학교가서 집에다가 좀 데려다 주세요 사장님 그런 개인적인 일은 ...</td>\n",
       "      <td>...</td>\n",
       "      <td>너 어제 집에 바로 안 들어갔지? 어떻게 알았어? 내가 눈이 좋아 왜 묻는건데? 너...</td>\n",
       "      <td>철수야 말 들어야지 싫은데 철밥통 선생아 너 선생님한테 말버릇이 뭐야 야 내가 너말...</td>\n",
       "      <td>동생이쁘다. 나주라 안돼 우리동생건들지마. 왜 안되냐? 좀 달라고 니 목숨값으로 동...</td>\n",
       "      <td>혹시 나 오늘 2만원만 빌려주만 안돼? 응? 안돼. 나도 요즘 맨날 택시타느라 택시...</td>\n",
       "      <td>이봐 네? 누구세요? 입다물고 가진거 다내놔 경찰에 신고할거에요 할 수 있으면 해봐...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...</td>\n",
       "      <td>야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  t_000  \\\n",
       "text  아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...   \n",
       "\n",
       "                                                  t_001  \\\n",
       "text  우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...   \n",
       "\n",
       "                                                  t_002  \\\n",
       "text  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...   \n",
       "\n",
       "                                                  t_004  \\\n",
       "text  아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...   \n",
       "\n",
       "                                                  t_005  \\\n",
       "text  그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...   \n",
       "\n",
       "                                                  t_006  \\\n",
       "text  그 사람 누구냐?  누구말하는거야?  다 알면서 모른척 하지마. 둘다 쏴버리기 전에...   \n",
       "\n",
       "                                                  t_007  \\\n",
       "text  야 저기야 닭꼬치 트럭왔다 응 그러네 그치? 너도 먹고 싶지? 어?나도? 그래 너 ...   \n",
       "\n",
       "                                                  t_009  \\\n",
       "text  야 너 2학년 김민석 맞지? 네 맞는데요. 혹시 누구신가요? 내가 누군지 궁금하면 ...   \n",
       "\n",
       "                                                  t_010  \\\n",
       "text  예전에 네가 나한테 했던 일 기억하지? 너도 당해봐 예전 일이라면 내가 사과할게. ...   \n",
       "\n",
       "                                                  t_012  ...  \\\n",
       "text  김비서 지금 우리애 학교가서 집에다가 좀 데려다 주세요 사장님 그런 개인적인 일은 ...  ...   \n",
       "\n",
       "                                                  t_489  \\\n",
       "text  너 어제 집에 바로 안 들어갔지? 어떻게 알았어? 내가 눈이 좋아 왜 묻는건데? 너...   \n",
       "\n",
       "                                                  t_490  \\\n",
       "text  철수야 말 들어야지 싫은데 철밥통 선생아 너 선생님한테 말버릇이 뭐야 야 내가 너말...   \n",
       "\n",
       "                                                  t_492  \\\n",
       "text  동생이쁘다. 나주라 안돼 우리동생건들지마. 왜 안되냐? 좀 달라고 니 목숨값으로 동...   \n",
       "\n",
       "                                                  t_493  \\\n",
       "text  혹시 나 오늘 2만원만 빌려주만 안돼? 응? 안돼. 나도 요즘 맨날 택시타느라 택시...   \n",
       "\n",
       "                                                  t_494  \\\n",
       "text  이봐 네? 누구세요? 입다물고 가진거 다내놔 경찰에 신고할거에요 할 수 있으면 해봐...   \n",
       "\n",
       "                                                  t_495  \\\n",
       "text  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...   \n",
       "\n",
       "                                                  t_496  \\\n",
       "text  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...   \n",
       "\n",
       "                                                  t_497  \\\n",
       "text  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...   \n",
       "\n",
       "                                                  t_498  \\\n",
       "text  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...   \n",
       "\n",
       "                                                  t_499  \n",
       "text  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....  \n",
       "\n",
       "[1 rows x 400 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_json(\"~/aiffel/dktc/data/test.json\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69f3bee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_000</th>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_001</th>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_002</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_004</th>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_005</th>\n",
       "      <td>그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "t_000  아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...\n",
       "t_001  우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...\n",
       "t_002  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...\n",
       "t_004  아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...\n",
       "t_005  그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.transpose()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e26a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [preprocess_sentence(s) for s in test_data[\"text\"]]\n",
    "tokenized_test_corpus = []\n",
    "for idx in range(len(test_corpus)):\n",
    "    tokenized_test_corpus.append(spm_tokenizer.EncodeAsIds(test_corpus[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e489deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(tokenized_test_corpus, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "139075b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6750f4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([400,   4], dtype=int32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03d8ed87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted = np.argmax(predicted, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eeaeacad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 3, 2, 0, 0, 3, 2, 2, 0, 3, 1, 3, 1, 1, 0, 0, 2, 2, 0,\n",
       "       3, 0, 1, 1, 1, 3, 3, 0, 3, 0, 2, 0, 2, 3, 1, 3, 0, 1, 0, 2, 2, 3,\n",
       "       1, 1, 2, 0, 3, 3, 1, 2, 3, 1, 1, 1, 2, 1, 2, 0, 2, 0, 2, 3, 3, 1,\n",
       "       2, 3, 0, 2, 2, 3, 1, 2, 2, 3, 3, 0, 3, 2, 2, 2, 1, 3, 3, 3, 2, 1,\n",
       "       0, 2, 2, 0, 2, 2, 1, 0, 0, 0, 2, 0, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1,\n",
       "       0, 1, 3, 3, 3, 3, 3, 0, 3, 1, 3, 2, 1, 3, 1, 3, 2, 2, 1, 3, 2, 2,\n",
       "       1, 2, 1, 2, 2, 0, 0, 2, 2, 2, 2, 3, 3, 1, 3, 3, 0, 1, 0, 1, 3, 3,\n",
       "       3, 2, 3, 1, 1, 3, 1, 2, 3, 2, 2, 1, 2, 2, 0, 3, 3, 0, 3, 1, 2, 3,\n",
       "       0, 3, 1, 3, 3, 1, 1, 1, 3, 0, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 3, 3,\n",
       "       3, 2, 2, 0, 2, 0, 2, 0, 1, 1, 1, 1, 3, 3, 2, 2, 2, 1, 2, 2, 0, 3,\n",
       "       1, 2, 2, 0, 2, 3, 1, 0, 0, 1, 1, 2, 1, 0, 3, 1, 3, 2, 0, 0, 2, 3,\n",
       "       0, 2, 3, 0, 3, 2, 1, 0, 1, 1, 3, 2, 3, 3, 0, 0, 0, 1, 2, 1, 3, 3,\n",
       "       3, 1, 0, 1, 0, 1, 3, 2, 3, 3, 0, 2, 0, 3, 3, 3, 1, 0, 0, 2, 1, 1,\n",
       "       3, 1, 3, 0, 0, 2, 3, 2, 1, 1, 1, 2, 3, 2, 2, 1, 0, 1, 0, 0, 2, 3,\n",
       "       2, 2, 1, 1, 3, 3, 0, 1, 3, 0, 2, 0, 1, 3, 3, 3, 1, 1, 2, 2, 1, 1,\n",
       "       2, 0, 3, 1, 3, 2, 1, 1, 3, 0, 3, 1, 1, 1, 0, 3, 2, 3, 1, 3, 3, 3,\n",
       "       2, 1, 3, 2, 0, 2, 3, 2, 2, 1, 3, 2, 1, 0, 2, 2, 0, 3, 0, 0, 2, 2,\n",
       "       1, 1, 2, 3, 2, 1, 3, 3, 2, 0, 2, 1, 0, 3, 0, 1, 1, 1, 3, 0, 0, 2,\n",
       "       1, 0, 2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2cf95f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"~/aiffel/dktc/data/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da58b460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  class\n",
       "0     t_000    NaN\n",
       "1     t_001    NaN\n",
       "2     t_002    NaN\n",
       "3     t_004    NaN\n",
       "4     t_005    NaN"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "589a70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['class'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c168fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"hihi2.csv\", index=False) ## 32.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17e08f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889854b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
